{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbea909",
   "metadata": {},
   "source": [
    "# Trabalho 2 - Tópicos em IA\n",
    "#### Aluno: Matheus Bernard Mota\n",
    "#### RGA: 2022.1904.008-6\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321654a",
   "metadata": {},
   "source": [
    "\n",
    "## Eficácia de Diferentes Técnicas de Pooling na Detecção de Fake News\n",
    "\n",
    "Neste notebook, utilizaremos o modelo BERT pré-treinado em pt-br **BERTimbau** com uma camada linear para classificação.\n",
    "Para o Fine-tuning, utilizaremos o dataset de *fake-news* Fake.br corpus. A ideia é, a partir do modelo pré treinado, especializá-lo em: a partir de uma sequência (notícia), corretamente predizer se ela é verdadeira **(true)** ou falsa, **(fake)**.\n",
    "\n",
    "O código fonte foi modularizado para melhor organização, compreensão e manutenibilidade e um notebook explicativo foi posto para direcionar a execução do procedimento e análise. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bf0ab",
   "metadata": {},
   "source": [
    "## Procedimentos Iniciais\n",
    "1. Anexar a pasta .src com este notebook, para que ele tenha acesso aos seus módulos\n",
    "2. Realizar os imports iniciais\n",
    "3. Mostra o device aceito pelo torch (#visualizações_adicionais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10a2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#permite importar src\n",
    "\n",
    "import os, sys\n",
    "# import sys\n",
    "\n",
    "root_relative_path = '../'\n",
    "root = os.path.abspath(root_relative_path)\n",
    "\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e11e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importa tudo aquilo que importa\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from src.dataset_loader import DatasetLoader\n",
    "from src.custom_bertimbau_classifier import CustomBertimbauClassifier\n",
    "# from src.baseline_bertimbau_classifier import BaselineBertimbauClassifier\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    EarlyStoppingCallback  #adicionado após ver que, na validação com 2 épocas, após o step 450, o modelo piorou a loss\n",
    ")\n",
    "from src.fine_tuner import FineTuner\n",
    "import src.config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "172363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#physics are relative, but this code gonna be DETERMINISTIC BABY!\n",
    "random.seed(cfg.RANDOM_SEED) \n",
    "np.random.seed(cfg.RANDOM_SEED) \n",
    "torch.manual_seed(cfg.RANDOM_SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b36f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(cfg.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c42c38",
   "metadata": {},
   "source": [
    "## Carregando o Dataset de Notícias\n",
    "\n",
    "O dataset contém 7200 notícias, com labels num ratio de ~50%\n",
    "\n",
    "#### Under the hood\n",
    "\n",
    "O que este loader está fazendo?\n",
    "1. Acessando o arquivo **pre-processed.csv** de um clone do repositório do Fake.br. (está no .gitignore, em caso de configuração local, este detalhe deve ser levado em conta)\n",
    "2. Carregando um tokenizer, proveniente do próprio bertimbau, para traduzir o texto do corpus em tokens processados \n",
    "3. Separando as colunas dos textos e suas respectivas \"labels\", ou seja, classificação da veracidade da notícia\n",
    "4. Separando, destas colunas, uma porcentagem para treino, validação(ajuste manual de parâmetros) e teste\n",
    "5. transformando estes dados em tokens e preparando um dataset anexavel ao Trainer da biblioteca Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceb2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root, cfg.PATH_TO_DATASET)\n",
    "ds_loader = DatasetLoader(\n",
    "    path=path,\n",
    "    model_name=cfg.BERTIMBAU,\n",
    "    max_len=cfg.SEQ_LEN\n",
    ")\n",
    "\n",
    "#conjunto para teste, validação e treino\n",
    "train_dataset, val_dataset, test_dataset = (ds_loader\n",
    "                                            .load_dataset(seed=cfg.RANDOM_SEED)\n",
    "                                            .get_datasets())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9152d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  6969,   304,  ...,     0,     0,     0],\n",
      "        [  101,  2779, 22278,  ...,   877,   151,   102],\n",
      "        [  101,   599,   266,  ..., 22281,  6185,   102],\n",
      "        ...,\n",
      "        [  101,   146, 12764,  ...,  3573,  1640,   102],\n",
      "        [  101,  6171, 20027,  ...,     0,     0,     0],\n",
      "        [  101,  4485,  1797,  ...,     0,     0,     0]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]])}\n",
      "{'input_ids': tensor([  101,  6969,   304, 11489,   562,  1315,  1418,  2354, 22279, 20808,\n",
      "         6248,  7635, 22278, 12456,  1502,  1004, 12456,  4133,   820,  3354,\n",
      "          926, 22278,  5698,  2277,  2354, 22279, 13540,  3307, 22278,  1069,\n",
      "          964,   130,   376,  6592,   268,  2354, 22279,  7635, 22278,  2684,\n",
      "          774,  1114, 22279,  1341,  1089,  3354,  1485, 11489,   562,  1315,\n",
      "         1418,  2354, 22279, 20808,  6248,  3566,   246,  1768,   151, 22280,\n",
      "          344,   304,   636,   706,  2765,  6564,  2748,  3102, 20811,  2182,\n",
      "         1147,  9296,  2745,  5540,   793,  1790,  3724,  1169,   143,   644,\n",
      "         4706, 22278,   547,  8137,  2547,   593,  2354, 22279,  2547,   593,\n",
      "         1169,   143,   706,   370, 11304, 13301,  1375,   333, 22278,  1684,\n",
      "          636, 21881, 12512, 22278,  2767,  2354, 22279,   102,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 'labels': tensor(0)}\n",
      "<src.dataset_loader.FakeNewsDataset object at 0x0000020A7AACD310>\n"
     ]
    }
   ],
   "source": [
    "#apenas para checar se tá tudo ok\n",
    "print(train_dataset.encodings)\n",
    "print(train_dataset[0])\n",
    "print(repr(train_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0aa11",
   "metadata": {},
   "source": [
    "## Carregando modelos BERTimbau\n",
    "\n",
    "#### **baseline_model**\n",
    "Versão do modelo onde o pooling é feito pela passagem do token **[CLS]**, presente no início de cada sentença, de modo a capturar o contexto geral de todo o documento. Este vetor é utilizado como entrada na camada de classificação.\n",
    "\n",
    "#### **alternative_model**\n",
    "Similarmente ao **baseline_model**, este modelo também aproveita do contexto capturado pelo token **[CLS]**, mas ele é concatenado com um vetor que calcula a média dos valores de todos os tokens presentes em cada sentença. Ou seja, se tenho X senteças de tamanho n  (os tamanhos variam, mas isso é regularizado com o token [PAD], que está configurado para não ser levado em consideração nas manipulações matriciais), cada i-ésimo token dos n tokens de cada sentença é somado X vezes, e uma média é retirada, divindindo o valor pelos numero de tokens significativos somados. Essa é outra forma de realizar o pooling, e garante um vetor de tamanho igual a de [CLS]. Estes dois vetores são então concatenados (dobrando o tamanho das features) e levado para a camada de classificação.\n",
    "\n",
    "$$\\text{mean\\_pooling}(\\mathbf{H}, \\mathbf{M}) = \\frac{\\sum_{i=1}^{n} \\mathbf{h}_i \\cdot m_i}{\\sum_{i=1}^{n} m_i}$$\n",
    "\n",
    "onde:\n",
    "- $\\mathbf{H}$ é a matriz de embedding\n",
    "- $\\mathbf{M}$ é a matriz de atenção, responsável por eliminar os tokens [PAD] da conta \n",
    "\n",
    "$$\\text{concat\\_pooling}(\\mathbf{H}, \\mathbf{M}) = [\\mathbf{h}_{[CLS]}; \\text{mean\\_pooling}(\\mathbf{H}, \\mathbf{M})]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b74fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = ds_loader.get_labels_mapping()\n",
    "\n",
    "alternative_model = CustomBertimbauClassifier(cfg.BERTIMBAU, **model_kwargs).to(cfg.DEVICE)\n",
    "# baseline_model = BaselineBertimbauClassifier(cfg.BERTIMBAU, **model_kwargs).to(cfg.DEVICE)\n",
    "baseline_model = (BertForSequenceClassification\n",
    "                    .from_pretrained(\n",
    "                        cfg.BERTIMBAU,\n",
    "                        **model_kwargs\n",
    "                    )\n",
    "                    .to(cfg.DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d32a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraç~ao dos Modelos:\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "___________________________________________\n",
      "CustomBertimbauClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1536, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Configuraç~ao dos Modelos:\")\n",
    "print(baseline_model)\n",
    "print(\"___________________________________________\")\n",
    "print(alternative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd0d0a",
   "metadata": {},
   "source": [
    "## Fine tunando os modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72642456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADICIONANDO AS MÉTRICAS SUGERIDAS NA AVALIAÇÃO\n",
    "#------------------------------------------------------\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "prec_metric = evaluate.load(\"precision\")\n",
    "rec_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": prec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": rec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "#EARLY STOPPING CALLBACK TECHNIQUE, \n",
    "# to prevent loss increasing (occured as said right above)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback( early_stopping_patience=2 ) \n",
    "\n",
    "#FINE TUNING PIPELINE\n",
    "#trouxe o pipeline para fora da classe, pois é uma peça importante que vale a pena estar a mostra\n",
    "#------------------------------------------------------\n",
    "def fine_tuning_pipeline(fine_tuner: FineTuner, model: torch.nn.Module, output_dir: str):\n",
    "    global root, train_dataset, val_dataset, compute_metrics, cfg, early_stopping_callback\n",
    "    return (\n",
    "        fine_tuner\n",
    "        .set_model(model)\n",
    "        .set_compute_metrics(compute_metrics)\n",
    "        .set_trainer_optimizer_params(lr_bert=cfg.LEARNING_RATE)\n",
    "        .set_training_arguments(\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True, #adicionado para o early stopping\n",
    "            load_best_model_at_end=True, #adicionado para o early stopping\n",
    "            output_dir=os.path.join(root, output_dir),\n",
    "            evaluation_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=cfg.LEARNING_RATE,\n",
    "            weight_decay=0.01,\n",
    "            per_device_train_batch_size=cfg.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=cfg.BATCH_SIZE,\n",
    "            num_train_epochs=cfg.NUM_EPOCHS,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_steps=90,\n",
    "            report_to=\"none\",\n",
    "            seed=cfg.RANDOM_SEED,\n",
    "        )\n",
    "        .set_trainer(\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks=[early_stopping_callback],\n",
    "        )\n",
    "        .train())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28f46450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fine_tuner = FineTuner(\n",
    "    tokenizer=ds_loader.get_tokenizer())\n",
    "\n",
    "tuned_baseline_res = fine_tuning_pipeline(\n",
    "    output_dir=\"bertimbau-baseline-cls-ptbr\",\n",
    "    fine_tuner=fine_tuner,\n",
    "    model=baseline_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab341b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuned_baseline_res' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtuned_baseline_res\u001b[49m\n",
      "\u001b[31mNameError\u001b[39m: name 'tuned_baseline_res' is not defined"
     ]
    }
   ],
   "source": [
    "tuned_baseline_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b68fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Aviso] parâmetros ignorados nesta versão: ['evaluation_strategy']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 4:04:43, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.588100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.376800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.250800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.203500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.099600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>540</td>\n",
       "      <td>0.109800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>630</td>\n",
       "      <td>0.108900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 03:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#bem que eu poderia criar um método de reset\n",
    "fine_tuner = FineTuner(\n",
    "    tokenizer=ds_loader.get_tokenizer())\n",
    "\n",
    "tuned_alternative_res =  fine_tuning_pipeline(\n",
    "    fine_tuner=fine_tuner,\n",
    "    model=alternative_model,\n",
    "    output_dir=\"bertimbau-alternativo-cls-ptbr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d7b924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.21633681654930115,\n",
       " 'eval_accuracy': 0.9481481481481482,\n",
       " 'eval_precision': 0.9483018867924529,\n",
       " 'eval_recall': 0.9481481481481482,\n",
       " 'eval_f1': 0.9481437023064392,\n",
       " 'eval_runtime': 190.118,\n",
       " 'eval_samples_per_second': 5.681,\n",
       " 'eval_steps_per_second': 0.358,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_alternative_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
