{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4cbea909",
   "metadata": {},
   "source": [
    "# Trabalho 2 - Tópicos em IA\n",
    "#### Aluno: Matheus Bernard Mota\n",
    "#### RGA: 2022.1904.008-6\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f321654a",
   "metadata": {},
   "source": [
    "\n",
    "## Eficácia de Diferentes Técnicas de Pooling na Detecção de Fake News\n",
    "\n",
    "Neste notebook, utilizaremos o modelo BERT pré-treinado em pt-br **BERTimbau** com uma camada linear para classificação.\n",
    "Para o Fine-tuning, utilizaremos o dataset de *fake-news* Fake.br corpus. A ideia é, a partir do modelo pré treinado, especializá-lo em: a partir de uma sequência (notícia), corretamente predizer se ela é verdadeira **(true)** ou falsa, **(fake)**.\n",
    "\n",
    "O código fonte foi modularizado para melhor organização, compreensão e manutenibilidade e um notebook explicativo foi posto para direcionar a execução do procedimento e análise. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01bf0ab",
   "metadata": {},
   "source": [
    "## Procedimentos Iniciais\n",
    "1. Anexar a pasta .src com este notebook, para que ele tenha acesso aos seus módulos\n",
    "2. Realizar os imports iniciais\n",
    "3. Mostra o device aceito pelo torch (#visualizações_adicionais)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b10a2a04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#permite importar src\n",
    "\n",
    "import os, sys\n",
    "# import sys\n",
    "\n",
    "root_relative_path = '../'\n",
    "root = os.path.abspath(root_relative_path)\n",
    "\n",
    "sys.path.append(root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "82e11e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#importa tudo aquilo que importa\n",
    "import torch\n",
    "import random\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "from src.dataset_loader import DatasetLoader\n",
    "from src.custom_bertimbau_classifier import CustomBertimbauClassifier\n",
    "# from src.baseline_bertimbau_classifier import BaselineBertimbauClassifier\n",
    "from transformers import (\n",
    "    BertForSequenceClassification, \n",
    "    EarlyStoppingCallback  #adicionado após ver que, na validação com 2 épocas, após o step 450, o modelo piorou a loss\n",
    ")\n",
    "from src.fine_tuner import FineTuner\n",
    "import src.config as cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "172363e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#physics are relative, but this code gonna be DETERMINISTIC BABY!\n",
    "random.seed(cfg.RANDOM_SEED) \n",
    "np.random.seed(cfg.RANDOM_SEED) \n",
    "torch.manual_seed(cfg.RANDOM_SEED)\n",
    "\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f4b36f27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(cfg.DEVICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c42c38",
   "metadata": {},
   "source": [
    "## Carregando o Dataset de Notícias\n",
    "\n",
    "O dataset contém 7200 notícias, com labels num ratio de ~50%\n",
    "\n",
    "#### Under the hood\n",
    "\n",
    "O que este loader está fazendo?\n",
    "1. Acessando o arquivo **pre-processed.csv** de um clone do repositório do Fake.br. (está no .gitignore, em caso de configuração local, este detalhe deve ser levado em conta)\n",
    "2. Carregando um tokenizer, proveniente do próprio bertimbau, para traduzir o texto do corpus em tokens processados \n",
    "3. Separando as colunas dos textos e suas respectivas \"labels\", ou seja, classificação da veracidade da notícia\n",
    "4. Separando, destas colunas, uma porcentagem para treino, validação(ajuste manual de parâmetros) e teste\n",
    "5. transformando estes dados em tokens e preparando um dataset anexavel ao Trainer da biblioteca Transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ceb2c7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = os.path.join(root, cfg.PATH_TO_DATASET)\n",
    "ds_loader = DatasetLoader(\n",
    "    path=path,\n",
    "    model_name=cfg.BERTIMBAU,\n",
    "    max_len=cfg.SEQ_LEN\n",
    ")\n",
    "\n",
    "#conjunto para teste, validação e treino\n",
    "train_dataset, val_dataset, test_dataset = (ds_loader\n",
    "                                            .load_dataset(seed=cfg.RANDOM_SEED)\n",
    "                                            .get_datasets())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9152d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[  101,  1224,  2413,  ...,   524,   516,   102],\n",
      "        [  101,  4004, 22278,  ...,  3554,   924,   102],\n",
      "        [  101,  1979,   593,  ..., 12219,   229,   102],\n",
      "        ...,\n",
      "        [  101,  1368,  6554,  ...,     0,     0,     0],\n",
      "        [  101,  1027,  1101,  ...,   393,   711,   102],\n",
      "        [  101,  4363, 22283,  ..., 10588, 22281,   102]]), 'token_type_ids': tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0],\n",
      "        [0, 0, 0,  ..., 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 1],\n",
      "        [1, 1, 1,  ..., 1, 1, 1]])}\n",
      "{'input_ids': tensor([  101,  1224,  2413, 20972,   516, 12440, 22283,  3887,   498,  5212,\n",
      "         1945,   966,  1359,   160, 22292,  3344, 14208,   342,  3557, 14580,\n",
      "          203, 16358,  1075,   734,   154,  2177, 14125,  7862,   629, 22280,\n",
      "         6844,   481,  1224,  2413, 20972,   516, 12440, 22283, 10481,  2821,\n",
      "         4764, 22282,  2940, 22232, 22278,  1945,   966,   734,   154,  2177,\n",
      "          449,   367,  1548, 16713, 14190,  5686,  3822,   726,  4251,  6514,\n",
      "        22278, 14125,  7862,   629, 22280,  4735, 22280,  3557, 14580,   203,\n",
      "          498, 13569,  4933,   622,  3001, 22287,   316, 22280,  1004,  1069,\n",
      "         3339,  4012,   449,   367,  1548,   516, 12440, 22283,  8954,  4654,\n",
      "          304,  6475,   547,  9745, 22232, 22278,  1945,   966,  3321, 12637,\n",
      "         1368,  3908,  1710,   223, 22289,  7134,  2021, 17732,  5866,   478,\n",
      "         4893,  7042,  8721,  1313,  6880,  4929,   138,  3096,  2208,   331,\n",
      "        21914, 13517,   481,  4031,   584, 18304,   139,  2014,   211,   586,\n",
      "         2293,   940,  4759,  2353,  6514, 22278,  1945,   966,  3898,  3388,\n",
      "          323,  4228,  1589,  2821, 13363,   232, 11297,   524,   516,   102]), 'token_type_ids': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0]), 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1]), 'labels': tensor(0)}\n",
      "<src.dataset_loader.FakeNewsDataset object at 0x0000015E1ECD7230>\n"
     ]
    }
   ],
   "source": [
    "#apenas para checar se tá tudo ok\n",
    "print(train_dataset.encodings)\n",
    "print(train_dataset[0])\n",
    "print(repr(train_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5fcf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "salvar_test_dataset = False\n",
    "if salvar_test_dataset:\n",
    "    import pickle\n",
    "\n",
    "    with open(os.path.join(root, 'documents', 'test_dataset.pkl'), 'wb') as f:\n",
    "        pickle.dump(test_dataset, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd0aa11",
   "metadata": {},
   "source": [
    "## Carregando modelos BERTimbau\n",
    "\n",
    "#### **baseline_model**\n",
    "Versão do modelo onde o pooling é feito pela passagem do token **[CLS]**, presente no início de cada sentença, de modo a capturar o contexto geral de todo o documento. Este vetor é utilizado como entrada na camada de classificação.\n",
    "\n",
    "#### **alternative_model**\n",
    "Similarmente ao **baseline_model**, este modelo também aproveita do contexto capturado pelo token **[CLS]**, mas ele é concatenado com um vetor que calcula a média dos valores de todos os tokens presentes em cada sentença. Ou seja, se tenho X senteças de tamanho n  (os tamanhos variam, mas isso é regularizado com o token [PAD], que está configurado para não ser levado em consideração nas manipulações matriciais), cada i-ésimo token dos n tokens de cada sentença é somado X vezes, e uma média é retirada, divindindo o valor pelos numero de tokens significativos somados. Essa é outra forma de realizar o pooling, e garante um vetor de tamanho igual a de [CLS]. Estes dois vetores são então concatenados (dobrando o tamanho das features) e levado para a camada de classificação.\n",
    "\n",
    "$$\\text{mean\\_pooling}(\\mathbf{H}, \\mathbf{M}) = \\frac{\\sum_{i=1}^{n} \\mathbf{h}_i \\cdot m_i}{\\sum_{i=1}^{n} m_i}$$\n",
    "\n",
    "onde:\n",
    "- $\\mathbf{H}$ é a matriz de embedding\n",
    "- $\\mathbf{M}$ é a matriz de atenção, responsável por eliminar os tokens [PAD] da conta \n",
    "\n",
    "$$\\text{concat\\_pooling}(\\mathbf{H}, \\mathbf{M}) = [\\mathbf{h}_{[CLS]}; \\text{mean\\_pooling}(\\mathbf{H}, \\mathbf{M})]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b74fcbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at neuralmind/bert-base-portuguese-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_kwargs = ds_loader.get_labels_mapping()\n",
    "\n",
    "alternative_model = CustomBertimbauClassifier(cfg.BERTIMBAU, **model_kwargs).to(cfg.DEVICE)\n",
    "# baseline_model = BaselineBertimbauClassifier(cfg.BERTIMBAU, **model_kwargs).to(cfg.DEVICE)\n",
    "baseline_model = (BertForSequenceClassification\n",
    "                    .from_pretrained(\n",
    "                        cfg.BERTIMBAU,\n",
    "                        **model_kwargs\n",
    "                    )\n",
    "                    .to(cfg.DEVICE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "04d32a17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuraç~ao dos Modelos:\n",
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n",
      "___________________________________________\n",
      "CustomBertimbauClassifier(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(29794, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0-11): 12 x BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSdpaSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (intermediate_act_fn): GELUActivation()\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (classifier): Linear(in_features=1536, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Configuraç~ao dos Modelos:\")\n",
    "print(baseline_model)\n",
    "print(\"___________________________________________\")\n",
    "print(alternative_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64bd0d0a",
   "metadata": {},
   "source": [
    "## Fine tunando os modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d9dbfd4",
   "metadata": {},
   "source": [
    "### Lógica Geral do Fine-tuning\n",
    "\n",
    "Aqui, na forma de um builder, está o pipeline seguido pelo fine-tuner até o treinamento.\n",
    "Os argumentos foram alterados para suportar a versão customizada do bertimbau, e algumas técnicas foram utilizadas visto comportamento dos treinos anteriores...\n",
    "Infelizmente, o CUDA não está disponível na máquina do autor, então 2 épocas foram utilizadas, em vez de três, que seria o padrão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "72642456",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ADICIONANDO AS MÉTRICAS SUGERIDAS NA AVALIAÇÃO\n",
    "#------------------------------------------------------\n",
    "acc_metric = evaluate.load(\"accuracy\")\n",
    "prec_metric = evaluate.load(\"precision\")\n",
    "rec_metric = evaluate.load(\"recall\")\n",
    "f1_metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": acc_metric.compute(predictions=preds, references=labels)[\"accuracy\"],\n",
    "        \"precision\": prec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"precision\"],\n",
    "        \"recall\": rec_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"recall\"],\n",
    "        \"f1\": f1_metric.compute(predictions=preds, references=labels, average=\"macro\")[\"f1\"]\n",
    "    }\n",
    "\n",
    "#EARLY STOPPING CALLBACK TECHNIQUE, \n",
    "# to prevent loss increasing (occured as said right above)\n",
    "\n",
    "early_stopping_callback = EarlyStoppingCallback( early_stopping_patience=2 ) \n",
    "\n",
    "#FINE TUNING PIPELINE\n",
    "#trouxe o pipeline para fora da classe, pois é uma peça importante que vale a pena estar a mostra\n",
    "#------------------------------------------------------\n",
    "def fine_tuning_pipeline(fine_tuner: FineTuner, model: torch.nn.Module, output_dir: str):\n",
    "    global root, train_dataset, val_dataset, compute_metrics, cfg, early_stopping_callback\n",
    "    return (\n",
    "        fine_tuner\n",
    "        .set_model(model)\n",
    "        .set_compute_metrics(compute_metrics)\n",
    "        .set_trainer_optimizer_params(lr_bert=cfg.LEARNING_RATE)\n",
    "        .set_training_arguments(\n",
    "            metric_for_best_model=\"f1\",\n",
    "            greater_is_better=True, #adicionado para o early stopping\n",
    "            load_best_model_at_end=True, #adicionado para o early stopping\n",
    "            output_dir=os.path.join(root,\"documents\",output_dir),\n",
    "            eval_strategy=\"epoch\",\n",
    "            save_strategy=\"epoch\",\n",
    "            learning_rate=cfg.LEARNING_RATE,\n",
    "            weight_decay=0.01,\n",
    "            per_device_train_batch_size=cfg.BATCH_SIZE,\n",
    "            per_device_eval_batch_size=cfg.BATCH_SIZE,\n",
    "            num_train_epochs=cfg.NUM_EPOCHS,\n",
    "            fp16=torch.cuda.is_available(),\n",
    "            logging_steps=90,\n",
    "            report_to=\"all\",\n",
    "            seed=cfg.RANDOM_SEED,\n",
    "        )\n",
    "        .set_trainer(\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=val_dataset,\n",
    "            callbacks=[early_stopping_callback],\n",
    "        )\n",
    "        .train())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be12e7d7",
   "metadata": {},
   "source": [
    "## Treinando modelo baseline \n",
    "(apenas um BertForSequenceClassification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "28f46450",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\src\\fine_tuner.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  self._trainer = Trainer(\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 2:43:06, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.251100</td>\n",
       "      <td>0.299417</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.904474</td>\n",
       "      <td>0.888889</td>\n",
       "      <td>0.887808</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.128100</td>\n",
       "      <td>0.220275</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.947224</td>\n",
       "      <td>0.947222</td>\n",
       "      <td>0.947222</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='68' max='68' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [68/68 03:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fine_tuner = FineTuner(\n",
    "    tokenizer=ds_loader.get_tokenizer())\n",
    "\n",
    "tuned_baseline_res = fine_tuning_pipeline(\n",
    "    output_dir= \"bertimbau-baseline-cls-ptbr\",\n",
    "    fine_tuner=fine_tuner,\n",
    "    model=baseline_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ab341b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.2202746421098709,\n",
       " 'eval_accuracy': 0.9472222222222222,\n",
       " 'eval_precision': 0.9472237559113714,\n",
       " 'eval_recall': 0.9472222222222222,\n",
       " 'eval_f1': 0.9472221769737457,\n",
       " 'eval_runtime': 205.0929,\n",
       " 'eval_samples_per_second': 5.266,\n",
       " 'eval_steps_per_second': 0.332,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_baseline_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ff265a",
   "metadata": {},
   "source": [
    "## Treinando o Modelo Alternativo\n",
    "Segue o mesmo pipeline, mas com as alterações necessárias sobreescritas na função forward do modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6b68fac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\src\\fine_tuner.py:61: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  self._trainer = Trainer(\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='630' max='630' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [630/630 11:29:44, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.250800</td>\n",
       "      <td>0.265315</td>\n",
       "      <td>0.904630</td>\n",
       "      <td>0.914420</td>\n",
       "      <td>0.904630</td>\n",
       "      <td>0.904063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.118000</td>\n",
       "      <td>0.214279</td>\n",
       "      <td>0.950926</td>\n",
       "      <td>0.951274</td>\n",
       "      <td>0.950926</td>\n",
       "      <td>0.950916</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#bem que eu poderia criar um método de reset\n",
    "fine_tuner = FineTuner(\n",
    "    tokenizer=ds_loader.get_tokenizer())\n",
    "\n",
    "tuned_alternative_res =  fine_tuning_pipeline(\n",
    "    fine_tuner=fine_tuner,\n",
    "    model=alternative_model,\n",
    "    output_dir=\"bertimbau-alternativo-cls-ptbr\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6d7b924e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 0.21427933871746063,\n",
       " 'eval_accuracy': 0.950925925925926,\n",
       " 'eval_precision': 0.9512741312741313,\n",
       " 'eval_recall': 0.950925925925926,\n",
       " 'eval_f1': 0.9509164576500096,\n",
       " 'eval_runtime': 187.3011,\n",
       " 'eval_samples_per_second': 5.766,\n",
       " 'eval_steps_per_second': 0.363,\n",
       " 'epoch': 2.0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tuned_alternative_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90310e1f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Realizando as **Predições!!**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "550987af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mathe\\OneDrive\\Área de Trabalho\\fake-news-detector\\.venv\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "preds = fine_tuner._trainer.predict(test_dataset) \n",
    "#eu não sabia que deveria pegar o predict assim, então não fiz o getter pro _trainer e violei o encapsulamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69f4c513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'test_loss': 0.18827331066131592,\n",
       " 'test_accuracy': 0.9564814814814815,\n",
       " 'test_precision': 0.9567461937817185,\n",
       " 'test_recall': 0.9564814814814815,\n",
       " 'test_f1': 0.9564751751582663,\n",
       " 'test_runtime': 297.7294,\n",
       " 'test_samples_per_second': 3.627,\n",
       " 'test_steps_per_second': 0.228}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7a608cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_tuner._trainer.save_model(os.path.join(root, \"documents\", \"model\",\"bertimbau-alternativo\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
